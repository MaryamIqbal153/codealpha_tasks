# -*- coding: utf-8 -*-
"""FinalLab_AI153.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dKUBVbXHUg39soOIP9hmfu6jwyyRXeJn

**QUESTION 1: MACHINE LEARNING (Titanic Dataset)**

**STEP 1: Import Required Libraries**
"""

import pandas as pd
import numpy as np
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

"""**STEP 2: Load Titanic Dataset**"""

titanic = fetch_openml('titanic', version=1, as_frame=True)
df = titanic.frame
df.head()

"""**STEP 3: Select Required Coloumns only**

**(a) Handle Missing Values**
"""

df = df[['age', 'fare', 'sex', 'embarked', 'survived']]

"""**STEP 4: Handle Missing Values (IMPORTANT)**"""

df['age'].fillna(df['age'].mean(), inplace=True)
df['fare'].fillna(df['fare'].mean(), inplace=True)
df['embarked'].fillna(df['embarked'].mode()[0], inplace=True)

df.isnull().sum()

"""**STEP 5: Encode Categorical Features**"""

df = pd.get_dummies(df, columns=['sex', 'embarked'], drop_first=True)

"""Step 6: Split Features and Target"""

X = df.drop('survived', axis=1)
y = df['survived']

y = y.astype(int)

"""Step 7: Normalize Data"""

scaler = StandardScaler()
X = scaler.fit_transform(X)

"""Step 8: Train-Test Split"""

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

"""Step 9: Train ML Model (Logistic Regression)"""

model = LogisticRegression()
model.fit(X_train, y_train)

"""Step 10: Predictions"""

y_pred = model.predict(X_test)

"""Step 11: Evaluation Metrics"""

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

"""Step 12: Display Results in DataFrame"""

results_df = pd.DataFrame({
    'Accuracy': [accuracy],
    'Precision': [precision],
    'Recall': [recall],
    'F1-Score': [f1]
})

results_df

"""**QUESTION 2: DEEP LEARNING (CIFAR-10)**

STEP 1: Import Libraries
"""

import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

"""STEP 2: Load Dataset"""

(X_train, y_train), (X_test, y_test) = cifar10.load_data()

"""STEP 3: Normalize Data"""

X_train = X_train / 255.0
X_test = X_test / 255.0

"""STEP 4: Build CNN Model"""

model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),
    MaxPooling2D(2,2),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

"""STEP 5: Compile Model"""

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

"""STEP 6: Train Model"""

history = model.fit(
    X_train, y_train,
    epochs=10,
    validation_data=(X_test, y_test)
)

"""STEP 7: Plot Graphs"""

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.show()

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.show()

"""STEP 8: Save Model"""

model.save("cifar10_cnn_model.h5")

"""**QUESTION 3: HYBRID MODEL (ANN + Random Forest)**

STEP 1: Feature Extraction Using ANN
"""

from tensorflow.keras.layers import Input
from tensorflow.keras.models import Model

ann = Sequential([
    Flatten(input_shape=(32,32,3)),
    Dense(128, activation='relu')
])

features = ann.predict(X_train)

"""STEP 2: Train Random Forest"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()
rf.fit(features, y_train.flatten())

"""STEP 3: Evaluation"""

test_features = ann.predict(X_test)
y_pred_rf = rf.predict(test_features)

from sklearn.metrics import confusion_matrix

print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))

"""STEP 4: Comparison (WRITE IN PDF)

Hybrid model showed better feature representation compared to CNN alone due to ANN-based feature extraction combined with Random Forest classification.
"""